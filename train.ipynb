{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvisionのdatasetを使ってUFC101を読み込み，pytorchvideoのx3dモデルをスクラッチで学習してみる．\n",
    "\n",
    "UFC101はあらかじめダウンロードして展開済みであるとする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import UCF101\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparseを真似たパラメータ設定．\n",
    "- rootで指定したディレクトリには，101クラスのサブディレクトリがあること\n",
    "- annotation_pathにはtrainlist0{1,2,3}.txtなどがあること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.metadata_path = '/mnt/HDD10TB/dataset/UFC101/'\n",
    "        self.root = '/mnt/HDD10TB/dataset/UFC101/video/'\n",
    "        self.annotation_path = '/mnt/HDD10TB/dataset/UFC101/ucfTrainTestlist/'\n",
    "        self.frames_per_clip = 16\n",
    "        self.step_between_clips = 16\n",
    "        self.model = 'X3D'\n",
    "        self.batch_size = 64\n",
    "        self.num_workers = 24\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformの定義．\n",
    "- UCF101を読み込むとuint8なので，255で割ってfloatにする．\n",
    "- torchvisionのUCF101データセットは(T, H, W, C)の形式．しかしpytorchvideoのx3dの入力形式は(B, C, T, H, W)らしいので，それに合わせる．\n",
    "- X3D-Mを想定して，画像を224x224にリサイズする．transform.Resize()はまだ試していないが，この形式ができるかどうか不明（torchvisionのtransformは画像しか扱わないのでムリだと思う）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x / 255.),\n",
    "    # (T, H, W, C) --> (C, T, H, W)\n",
    "    transforms.Lambda(lambda x: x.permute(3, 0, 1, 2)),\n",
    "    transforms.Lambda(\n",
    "        lambda x: nn.functional.interpolate(x, (224, 224))),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットはimage, audio, labelの三組を返すが，UCF101には音声がない動画もあり，そのまま使うとdataloaderがバッチにできないというエラーが出てしまう（audioの次元数がサンプルによって異なるため）．そこでcollateでaudioを取り除く．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_audio_collate(batch):\n",
    "    # https://www.kaggle.com/pevogam/starter-ucf101-with-pytorch\n",
    "    '''\n",
    "    remove audio channel because\n",
    "    not all of UCF101 vidoes have audio channel\n",
    "    '''\n",
    "    video_only_batch = []\n",
    "    for video, audio, label in batch:\n",
    "        video_only_batch.append((video, label))\n",
    "    return default_collate(video_only_batch)\n",
    "\n",
    "custom_collate = remove_audio_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メタデータの準備．UCF101の全動画をスキャンして，FPSなどの情報を取得するらしい．かなり時間がかかる．\n",
    "それを保存して再利用（毎回計算し直すと時間の無駄）．\n",
    "コードを見たところ，foldやtrainには無関係で，fpcとsbcにだけ依存するらしいので，それをファイル名にして保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename = os.path.join(\n",
    "    args.metadata_path,\n",
    "    'UCF101metadata_fpc{}_sbc{}.pickle'.format(\n",
    "        args.frames_per_clip,\n",
    "        args.step_between_clips))\n",
    "\n",
    "if not os.path.exists(metadata_filename):\n",
    "    # precompute and save metadata\n",
    "    dataset_dict = UCF101(root=args.root,\n",
    "                            annotation_path=args.annotation_path,\n",
    "                            frames_per_clip=args.frames_per_clip,\n",
    "                            step_between_clips=args.step_between_clips,\n",
    "                            num_workers=args.num_workers,\n",
    "                            )\n",
    "    with open(metadata_filename, \"wb\") as f:\n",
    "        pickle.dump(dataset_dict.metadata, f)\n",
    "\n",
    "with open(metadata_filename, 'rb') as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCF101には3つのスプリットがあるので，foldでそれを指定（多分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = UCF101(root=args.root,\n",
    "                    annotation_path=args.annotation_path,\n",
    "                    frames_per_clip=args.frames_per_clip,\n",
    "                    step_between_clips=args.step_between_clips,\n",
    "                    fold=1,\n",
    "                    train=True,\n",
    "                    transform=transform,\n",
    "                    _precomputed_metadata=metadata)\n",
    "val_set = UCF101(root=args.root,\n",
    "                    annotation_path=args.annotation_path,\n",
    "                    frames_per_clip=args.frames_per_clip,\n",
    "                    step_between_clips=args.step_between_clips,\n",
    "                    fold=1,\n",
    "                    train=False,\n",
    "                    transform=transform,\n",
    "                    _precomputed_metadata=metadata)\n",
    "n_classes = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダーの作成．collateをここで指定．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collate,\n",
    "                            num_workers=args.num_workers)\n",
    "val_loader = DataLoader(val_set,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=custom_collate,\n",
    "                        num_workers=args.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchvideoのx3dモデルを作成．\n",
    "webマニュアルにはないが，コードをみると，クリップ長とサイズが指定できる．\n",
    "X3Dは数種類あるが，ここではX3D-Mに合わせた数字を指定（コードのコメントに書いてある）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3D-M\n",
    "# https://github.com/facebookresearch/pytorchvideo/blob/master/pytorchvideo/models/x3d.py#L601\n",
    "model = x3d.create_x3d(\n",
    "    input_clip_length=16,\n",
    "    input_crop_size=224,\n",
    "    depth_factor=2.2,\n",
    "    model_num_class=101\n",
    ").to(device)\n",
    "\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムなデータを流し込んで出力されるかを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(2, 3, 16, 224, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0092, 0.0085, 0.0106, 0.0093, 0.0096, 0.0093, 0.0100, 0.0102, 0.0091,\n",
       "         0.0093, 0.0097, 0.0102, 0.0107, 0.0097, 0.0087, 0.0107, 0.0109, 0.0118,\n",
       "         0.0076, 0.0093, 0.0103, 0.0102, 0.0124, 0.0090, 0.0108, 0.0077, 0.0090,\n",
       "         0.0093, 0.0100, 0.0114, 0.0082, 0.0104, 0.0089, 0.0110, 0.0106, 0.0099,\n",
       "         0.0085, 0.0095, 0.0092, 0.0120, 0.0096, 0.0093, 0.0089, 0.0087, 0.0102,\n",
       "         0.0092, 0.0119, 0.0110, 0.0087, 0.0094, 0.0100, 0.0088, 0.0085, 0.0102,\n",
       "         0.0099, 0.0085, 0.0096, 0.0090, 0.0095, 0.0100, 0.0113, 0.0122, 0.0096,\n",
       "         0.0114, 0.0084, 0.0113, 0.0109, 0.0081, 0.0083, 0.0099, 0.0108, 0.0095,\n",
       "         0.0113, 0.0103, 0.0106, 0.0100, 0.0103, 0.0085, 0.0102, 0.0113, 0.0093,\n",
       "         0.0096, 0.0093, 0.0126, 0.0091, 0.0107, 0.0097, 0.0102, 0.0104, 0.0106,\n",
       "         0.0098, 0.0107, 0.0102, 0.0084, 0.0107, 0.0088, 0.0099, 0.0105, 0.0111,\n",
       "         0.0094, 0.0113],\n",
       "        [0.0131, 0.0102, 0.0098, 0.0092, 0.0095, 0.0095, 0.0103, 0.0113, 0.0088,\n",
       "         0.0097, 0.0091, 0.0087, 0.0106, 0.0113, 0.0095, 0.0116, 0.0110, 0.0101,\n",
       "         0.0091, 0.0090, 0.0090, 0.0100, 0.0084, 0.0097, 0.0101, 0.0094, 0.0089,\n",
       "         0.0107, 0.0100, 0.0105, 0.0103, 0.0094, 0.0080, 0.0109, 0.0098, 0.0094,\n",
       "         0.0090, 0.0096, 0.0113, 0.0114, 0.0092, 0.0097, 0.0091, 0.0084, 0.0104,\n",
       "         0.0083, 0.0116, 0.0104, 0.0083, 0.0099, 0.0091, 0.0100, 0.0096, 0.0093,\n",
       "         0.0093, 0.0098, 0.0084, 0.0086, 0.0084, 0.0102, 0.0095, 0.0105, 0.0084,\n",
       "         0.0097, 0.0093, 0.0107, 0.0119, 0.0110, 0.0086, 0.0096, 0.0120, 0.0111,\n",
       "         0.0086, 0.0113, 0.0083, 0.0109, 0.0090, 0.0079, 0.0093, 0.0098, 0.0111,\n",
       "         0.0100, 0.0092, 0.0097, 0.0106, 0.0123, 0.0098, 0.0117, 0.0109, 0.0104,\n",
       "         0.0103, 0.0100, 0.0085, 0.0097, 0.0104, 0.0097, 0.0096, 0.0106, 0.0098,\n",
       "         0.0118, 0.0118]], device='cuda:0', grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summaryで中身を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape               Output Shape\n",
       "===================================================================================================================\n",
       "DataParallel                                                      --                        --\n",
       "├─Net (module)                                                    --                        --\n",
       "├─Net (module)                                                    --                        --\n",
       "│    └─ModuleList (blocks)                                        --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─Conv2plus1d (conv)                               --                        --\n",
       "├─Net (module)                                                    --                        --\n",
       "├─Net (module)                                                    --                        --\n",
       "│    └─ModuleList (blocks)                                        --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─Conv2plus1d (conv)                               [1, 3, 16, 224, 224]      [1, 24, 16, 112, 112]\n",
       "├─Net (module)                                                    --                        --\n",
       "├─Net (module)                                                    --                        --\n",
       "│    └─ModuleList (blocks)                                        --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─BatchNorm3d (norm)                               [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─Conv2plus1d (conv)                               --                        --\n",
       "│    │    │    └─ReLU (activation)                                [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "├─Net (module)                                                    [1, 3, 16, 224, 224]      [1, 101]\n",
       "├─Net (module)                                                    --                        --\n",
       "│    └─ModuleList (blocks)                                        --                        --\n",
       "│    │    └─ResStage (1)                                          --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   [1, 3, 16, 224, 224]      [1, 24, 16, 112, 112]\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─Conv2plus1d (conv)                               [1, 3, 16, 224, 224]      [1, 24, 16, 112, 112]\n",
       "│    │    │    └─BatchNorm3d (norm)                               [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    │    └─ReLU (activation)                                [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    └─ResStage (1)                                          [1, 24, 16, 112, 112]     [1, 24, 16, 56, 56]\n",
       "│    │    └─ResStage (2)                                          --                        --\n",
       "│    │    └─ResStage (2)                                          [1, 24, 16, 56, 56]       [1, 48, 16, 28, 28]\n",
       "│    │    └─ResStage (3)                                          --                        --\n",
       "│    │    └─ResStage (3)                                          [1, 48, 16, 28, 28]       [1, 96, 16, 14, 14]\n",
       "│    │    └─ResStage (4)                                          --                        --\n",
       "│    │    └─ResStage (4)                                          [1, 96, 16, 14, 14]       [1, 192, 16, 7, 7]\n",
       "│    │    └─ResNetBasicHead (5)                                   [1, 192, 16, 7, 7]        [1, 101]\n",
       "│    │    └─ResNetBasicHead (5)                                   --                        --\n",
       "│    │    │    └─ProjectedPool (pool)                             [1, 192, 16, 7, 7]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Dropout (dropout)                                [1, 2048, 1, 1, 1]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Linear (proj)                                    [1, 1, 1, 1, 2048]        [1, 1, 1, 1, 101]\n",
       "│    │    │    └─Softmax (activation)                             [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    │    └─AdaptiveAvgPool3d (output_pool)                  [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    └─ResNetBasicHead (5)                                   [1, 192, 16, 7, 7]        [1, 101]\n",
       "│    │    └─ResNetBasicHead (5)                                   --                        --\n",
       "│    │    │    └─ProjectedPool (pool)                             [1, 192, 16, 7, 7]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Dropout (dropout)                                [1, 2048, 1, 1, 1]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Linear (proj)                                    [1, 1, 1, 1, 2048]        [1, 1, 1, 1, 101]\n",
       "│    │    │    └─Softmax (activation)                             [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    │    └─AdaptiveAvgPool3d (output_pool)                  [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─BatchNorm3d (norm)                               [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    │    └─ReLU (activation)                                [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    └─ResStage (1)                                          [1, 24, 16, 112, 112]     [1, 24, 16, 56, 56]\n",
       "│    │    └─ResStage (2)                                          [1, 24, 16, 56, 56]       [1, 48, 16, 28, 28]\n",
       "│    │    └─ResStage (3)                                          [1, 48, 16, 28, 28]       [1, 96, 16, 14, 14]\n",
       "│    │    └─ResStage (4)                                          [1, 96, 16, 14, 14]       [1, 192, 16, 7, 7]\n",
       "│    │    └─ResNetBasicHead (5)                                   --                        --\n",
       "│    │    └─ResNetBasicHead (5)                                   --                        --\n",
       "│    │    │    └─ProjectedPool (pool)                             [1, 192, 16, 7, 7]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Dropout (dropout)                                [1, 2048, 1, 1, 1]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Linear (proj)                                    --                        --\n",
       "│    │    └─ResNetBasicStem (0)                                   --                        --\n",
       "│    │    │    └─BatchNorm3d (norm)                               [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    │    └─ReLU (activation)                                [1, 24, 16, 112, 112]     [1, 24, 16, 112, 112]\n",
       "│    │    └─ResStage (1)                                          [1, 24, 16, 112, 112]     [1, 24, 16, 56, 56]\n",
       "│    │    └─ResStage (2)                                          [1, 24, 16, 56, 56]       [1, 48, 16, 28, 28]\n",
       "│    │    └─ResStage (3)                                          [1, 48, 16, 28, 28]       [1, 96, 16, 14, 14]\n",
       "│    │    └─ResStage (4)                                          [1, 96, 16, 14, 14]       [1, 192, 16, 7, 7]\n",
       "│    │    └─ResNetBasicHead (5)                                   [1, 192, 16, 7, 7]        [1, 101]\n",
       "│    │    └─ResNetBasicHead (5)                                   --                        --\n",
       "│    │    │    └─ProjectedPool (pool)                             [1, 192, 16, 7, 7]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Dropout (dropout)                                [1, 2048, 1, 1, 1]        [1, 2048, 1, 1, 1]\n",
       "│    │    │    └─Linear (proj)                                    [1, 1, 1, 1, 2048]        [1, 1, 1, 1, 101]\n",
       "│    │    │    └─Softmax (activation)                             [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    │    └─AdaptiveAvgPool3d (output_pool)                  [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    │    └─Softmax (activation)                             [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "│    │    │    └─AdaptiveAvgPool3d (output_pool)                  [1, 101, 1, 1, 1]         [1, 101, 1, 1, 1]\n",
       "===================================================================================================================\n",
       "Total params: 3,181,623\n",
       "Trainable params: 3,181,623\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.95\n",
       "===================================================================================================================\n",
       "Input size (MB): 38.54\n",
       "Forward/backward pass size (MB): 1271.71\n",
       "Params size (MB): 12.73\n",
       "Estimated Total Size (MB): 1322.97\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(\n",
    "    model,\n",
    "    (4, 3, 16, 224, 224),\n",
    "    depth=4,\n",
    "    col_names=[\"input_size\",\n",
    "               \"output_size\"],\n",
    "    row_settings=(\"var_names\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "便利関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvisionのvideo.pyで，ワーニングが多数出るのでそれを抑制．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                                   module='torchvision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    for epoch in pbar_epoch:\n",
    "        pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "\n",
    "        with tqdm(enumerate(train_loader),\n",
    "                  total=len(train_loader),\n",
    "                  leave=True) as pbar_loss:\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, (inputs, targets) in pbar_loss:\n",
    "                pbar_loss.set_description(\"[train]\")\n",
    "\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                bs = inputs.size(0)  # current batch size, may vary at the end of the epoch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.update(loss, bs)\n",
    "                train_acc.update(top1(outputs, targets), bs)\n",
    "\n",
    "                pbar_loss.set_postfix_str(\n",
    "                    ' | loss={:6.04f} , top1={:6.04f}'\n",
    "                    ''.format(\n",
    "                    train_loss.avg, train_acc.avg,\n",
    "                ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
